# Data Engineer Test Case

Objective: Develop an ETL pipeline that extracts public data from the internet, stores it in a data lake, cleans it, and persists it into a data warehouse using an orchestration framework, SQL scripting, and a data warehousing solution.

Data Source: Use the following public dataset of US COVID-19 cases provided by the New York Times: https://github.com/nytimes/covid-19-data/blob/master/us.csv

Task:

- Orchestration: Use an orchestration framework (e.g., Prefect, Apache Airflow, or Dagster) to automate the ETL pipeline.

- Extract: Within the orchestrator, download the "us.csv" file from the GitHub repository.

- Data Lake Storage: Store the extracted data in a data lake storage solution (e.g., Azure Blob, GCP Cloud Storage, S3, Google Drive, or Dropbox).

- Transform: Clean the data by:

  - Removing any rows with missing or invalid data.
  - Converting the date column to a standard date format (YYYY-MM-DD).
  - Calculating the daily new cases and deaths by taking the difference between consecutive rows.
  - Ensuring data quality by handling potential issues such as duplicates, incorrect data types, or inconsistencies.
  - Load: Persist the cleaned data into a data warehouse solution (e.g., BigQuery or PostgreSQL). Include the necessary schema and table creation scripts.

- SQL Scripting: Use a SQL scripting tool (e.g., dbt) to create a dbt model that generates a summary report, then persist the report back to the data warehouse. The summary report should include the following metrics:

  - Total cases and deaths per month.
  - The average daily new cases and deaths per month.
  - The top 5 days with the highest daily new cases and deaths.
- (Optional) Deployment: Package the workflow orchestration component in a Docker container for easy deployment.

- Upload the complete solution to a GitHub repository.

Deliverables:

- A link to the GitHub repository containing the well-structured project folder with all the code, scripts, and necessary files.
- A README.md file within the repository that explains the solution, its components, and instructions on how to run and deploy the pipeline.
- A sample summary report generated by the pipeline, including the metrics mentioned in Task 6.
Completion Time: 1 week

Evaluation Criteria:

- Code quality, readability, and organization.
- Proper implementation of the ETL pipeline and its components.
- The correctness and accuracy of the transformed data and summary report.
- (Optional) Successful deployment of the workflow orchestration component using Docker.
- Documentation clarity and completeness.
Good luck!
